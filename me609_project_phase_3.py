# -*- coding: utf-8 -*-
"""me609 project phase 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zyrEwh-XaMkV6UYrsvQmL-JTaNlLkkrH
"""

#-------------------------------------------------------------------------------------------------------
# ME609 Optimization Methods in Engineering Project Phase 3
# Shreerup Ikare (200103128) and Saarthak Sarkar (200103094)
# Conjugate Gradient Method using Bounding Phase Method and Bisection Method for unidirectional searches
# Bracket-operator Penalty Method
#-------------------------------------------------------------------------------------------------------

import numpy as np
import random
import math

pi = 3.141521
R = 0.1

def objective_function(x):                                      #Basic Objective function definition without penalty
    #return (x[0] - 10)**3 + (x[1] - 20)**3
    return -1 * (math.sin(2 * math.pi * x[0])**3 * math.sin(2 * math.pi * x[1])) / (x[0]**3 * (x[0] + x[1]))

def constraint_function_q1_1(x):                                #Constraints specified for all questions
    return (x[0]-5)**2 + (x[1]-5)**2 - 100

def constraint_function_q1_2(x):
    return 82.81 - (x[0]-6)**2 - (x[1]-5)**2

def constraint_function_q2_1(x):
    return -x[0]**2 + x[1] - 1

def constraint_function_q2_2(x):
    return -1 + x[0] - (x[1]-4)**2

R = 0.1
def penalty_function_q1(x, R):                              #Penalty function is sum of R<a(x)> where <x> = <x> if negative, else 0
    constraint_violations = [min(0, constraint_function_q1_1(x)), min(0, constraint_function_q1_2(x))]
    # Add more constraint violations for additional constraints if needed
    total_penalty = R * sum(violation**2 for violation in constraint_violations)
    return total_penalty

R = 0.1
def penalty_function_q2(x, R):
    constraint_violations = [min(0, constraint_function_q2_1(x)), min(0, constraint_function_q2_2(x))]
    # Add more constraint violations for additional constraints if needed
    total_penalty = R * sum(violation**2 for violation in constraint_violations)
    return total_penalty


# Combined objective function with penalty
def combined_objective(x):
    #return objective_function(x) + penalty_function_q1(x, R)
    return objective_function(x) + penalty_function_q2(x, R)


# Define the bounds for x
bounds = [(13, 20), (0, 4)]


def alpha_range(x, s0, x_l, x_u, n):
    alpha_lower  = np.zeros(n)
    alpha_upper  = np.zeros(n)
    for i in range(n):
        alpha_lower[i] = ((x_l[i]-x[i])/s0[i])            ## Finding alpha values from formula x(k+1) = x(k) + alpha * s(k)
        alpha_upper[i] = ((x_u[i]-x[i])/s0[i])
    return max(alpha_lower), min(alpha_upper)



def numerical_derivative(z, h, x, s):
    x_plus_h = [xi + (z + h) * si for xi, si in zip(x, s)]
    x_minus_h = [xi + (z - h) * si for xi, si in zip(x, s)]
    deri = (combined_objective(x_plus_h) - combined_objective(x_minus_h)) / (2 * h)
    return deri


def negative_gradient(x, h):                                  #function for calculating negative gradient at a given point x

    negative_grad = []
    for i in range(len(x)):
        x_plus_h = x[:]
        x_plus_h[i] += h                                     #create lists for (x+h) and (x-h)
        x_minus_h = x[:]
        x_minus_h[i] -= h
                                     #Now Append the negative gradient value calculated using fundamental derivative formula
        negative_grad.append((combined_objective(x_minus_h) - combined_objective(x_plus_h)) / (2 * h))
    return negative_grad


def dot_product(x, s0):                                   #Iterate over x and s0 vector to find the dot product
    return sum(-1 * d * s for d, s in zip(x, s0))


def mod_val(x):                                           #Find the norm of the vector x using mod_val(x) function
    return math.sqrt(sum(xi ** 2 for xi in x))



alpha1 = 0.0
aplha2 = 0.0
def bounding_phase_method(x, s0, n, output_file):
    #initial_guess = [(ai + bi) / 2 for ai, bi in zip(a, b)]
    initial_guess = x[:]                                            #consider initial guess as the the input taken from txt file
    delta = 0.05                                                    #Set delta as 0.05 (can be changed as required)
    direction = 1                                                   #The direction of delta is set positive initially. Will check it further

    step_size = [delta * (si / mod_val(s0)) for si in s0]           #As these are vectors, Make sure the increment is along the s0 direction
    x1 = [ig - ss for ig, ss in zip(initial_guess, step_size)]      #This is done by multiplying by unit vector in the desired direction
    x2 = [ig + ss for ig, ss in zip(initial_guess, step_size)]

    print("x1 and x2 values are set")
    print("x1: ", x1[0], x1[1])
    print("x2: ", x2[0], x2[1])
    if combined_objective(x1) >= combined_objective(initial_guess) and combined_objective(initial_guess) >= combined_objective(x2):
        pass
    else:                                                           #Check if the sign of delta needs to be updated
        direction = -1

    x_prev = initial_guess[:]
    x_current = [xp + direction * ss * (2 ** 0) for xp, ss in zip(x_prev, step_size)]

    k = 1                                                           #Set k equal to 1. Updates as per iteration
    x_old = x_prev[:]
    while combined_objective(x_current) < combined_objective(x_prev):
        x_old = x_prev[:]                                           #In next loop, old is updated to prev, prev to current and current as per the formula
        x_prev = x_current[:]
        x_current = [xp + direction * ss * (2 ** k) for xp, ss in zip(x_prev, step_size)]

        #For all iterations, we write the iteration number and the objective function value at x_prev in an output file
        output_file.write(f'k={k}, f(x_prev)={combined_objective(x_prev)}\n')
        output_file.write(f'k={k}, x_prev={x_prev}\n')
        k += 1                                                      #Update k


        print("x_old: ", x_old)
        print("x_prev: ", x_prev)                                   #Print values for our reference
        print("x_current: ", x_current)
        print("Function value at x_old: ", combined_objective(x_old))
        print("Function value x_prev: ", combined_objective(x_prev))
        print("Function value x_current: ", combined_objective(x_current))

    #x_next = [xp + direction * ss * (2 ** k) for xp, ss in zip(x_prev, step_size)]
    #x_current = [xp + direction * ss * (2 ** (k-1)) for xp, ss in zip(x_prev, step_size)]
    #x_prev = [xp + direction * ss * (2 ** (k-2)) for xp, ss in zip(x_prev, step_size)]

    print("optimized range of (x_old, x_current) is obtained")
    if direction == 1:

#         range_of_alpha = alpha_range(x, s0, x_old, x_current, n)
#         alpha1 = range_of_alpha[0]
#         alpha2 = range_of_alpha[1]

#         x_l = [13, 0]
#         x_u = [20, 4]
#         a_l, a_u = alpha_range(x, s0, x_l, x_u, n)

#         a_lower = min(a_l, a_u)
#         a_upper = max(a_l, a_u)

#         print("Alpha1: ", alpha1)
#         print("Alpha2: ", alpha2)
#         print("a_lower: ", a_lower)
#         print("a_upper: ", a_upper)

#         req_alpha_1 = max(alpha1, a_lower)
#         req_alpha_2 = min(alpha2, a_upper)

#         for i in range(n):
#             x_old[i] = x[i] + req_alpha_1*s0[i]
#             x_current[i] = x[i] + req_alpha_2*s0[i]

        #x_lower = [13, 0]
        #x_upper = [20, 4]
        #for i in range(n):
        #    x_old[i] = max(x_old[i], x_lower[i])
        #    x_old[i] = min(x_old[i], x_upper[i])
        #    x_current[i] = max(x_current[i], x_lower[i])
        #    x_current[i] = min(x_current[i], x_upper[i])

        print("Returned range: ", x_old, x_current)                #According to the direction the bounds are generated and returned
        return x_old, x_current
    else:

#         range_of_alpha = alpha_range(x, s0, x_current, x_old, n)
#         alpha1 = range_of_alpha[0]
#         alpha2 = range_of_alpha[1]

#         x_l = [13, 0]
#         x_u = [20, 4]
#         a_l, a_u = alpha_range(x, s0, x_l, x_u, n)

#         a_lower = min(a_l, a_u)
#         a_upper = max(a_l, a_u)

#         print("Alpha1: ", alpha1)
#         print("Alpha2: ", alpha2)
#         print("a_lower: ", a_lower)
#         print("a_upper: ", a_upper)

#         req_alpha_1 = max(alpha1, a_lower)
#         req_alpha_2 = min(alpha2, a_upper)

#         for i in range(n):
#             x_current[i] = x[i] + req_alpha_1*s0[i]
#             x_old[i] = x[i] + req_alpha_2*s0[i]

#         x_lower = [13, 0]
#         x_upper = [20, 4]
#         for i in range(n):
#             x_old[i] = max(x_old[i], x_lower[i])
#             x_old[i] = min(x_old[i], x_upper[i])
#             x_current[i] = max(x_current[i], x_lower[i])
#             x_current[i] = min(x_current[i], x_upper[i])

        print("Returned range: ", x_current, x_old)
        return x_current, x_old


def bisection_method(x1, x2, s0):               #Bisection Function takes input range (vectors) from Bounding Phase and the search dierction
    z = [0] * len(x1)                           #Initialize z as a zero vector. Will be updated as needed
    epsilon = 0.01

    while True:
        for i in range(len(x1)):
            z[i] = (x1[i] + x2[i]) / 2          #z vector is updated to the mean of the bound values

        dz = negative_gradient(z, 0.001)        #gradient is computed at the new z in each iteration

        #dz = [-1 * x for x in dz]
        #dot_dz = sum(-1 * d * s for d, s in zip(dz, s0))

        if abs(dot_product(dz, s0))/mod_val(s0) < epsilon:          #condition to break out of the loop
            break

        if dot_product(dz, s0) > 0:             #As these are vectors, we check the dot product of gradient with the search direction
            x2 = z[:]                           #and update x2 or x1 to z accordingly
        else:
            x1 = z[:]

        print("x1: ", x1)
        print("x2: ", x2)                      #print values for each iteration for our reference
        print("z: ", z)
        print("dx1 dot product: ", dot_product(negative_gradient(x1, 0.001), s0))
        print("dx2 dot product: ", dot_product(negative_gradient(x2, 0.001), s0))
        print("dz dot product: ", dot_product(dz, s0))
        print("condition value: ", abs(dot_product(dz, s0)) / mod_val(s0))
        print("z is updated")

    return z


def conjugate_gradient_method(n, x, output_file):          #conjugate gradient function

    epsilon = 0.01                                         #Set epsilon and k
    k = 0
    c = 10
    s0 = negative_gradient(x, 0.001)                      #Search direction is computed from the initial guessed point
    alpha1, alpha2 = 0.0, 0.0
    x_next = [0] * n
    s_next = [0] * n
    M = 100                                               #M is set to break the loop if number of iterations exceed M

    while (mod_val(negative_gradient(x, 0.001)) > epsilon and k<M):

        range_input = bounding_phase_method(x, s0, n, output_file)     #Store the range bounds that we get from Bounding Phase Method
        print("Bounding Phase Method completed")

        x_next = bisection_method(range_input[0], range_input[1], s0)     #The most optimal point in unidirectional search is obtained
        print("Bisection method completed")

        term1 = negative_gradient(x_next, 0.001)
        num = mod_val(negative_gradient(x_next, 0.001)) ** 2              #Numerator and Denominator values in second term
        den = mod_val(negative_gradient(x, 0.001)) ** 2
        s_next = [t1 + (num / den) * s for t1, s in zip(term1, s0)]       #New search direction is found using the formula
        x = x_next[:]                                                     #Update x and s0 for next iteration
        s0 = s_next[:]
        k=k+1                                                             #Increment k and print iteration number

        print("Iteration Number: ", k)
        print("x and s0 are updated")

    R=R*c
    return x                                           #Return the optimum point after termination condition is satisfied



if __name__ == "__main__":                            #Main function. Code starts executing from this point

    n = int(input("Enter number of variables: "))
    x = []                                                               #This is if you want input directly from the user
    for i in range(n):
        value = float(input(f"Enter the value of x[{i}]: "))
        x.append(value)

    #with open("input.txt", "r") as file:                                  #Taking input from a input text file
    #    n = int(file.readline())                                          # Read the number of variables
    #    x_values = list(map(float, file.readline().split()))              # Read the x vector as a list of floats

    with open("output.txt", "w") as output_file:
        ans = conjugate_gradient_method(n, x, output_file)                #Output file

    print("The final answer for most optimum solution is: ")
    for val in ans:
        print(val, end=" ")                                               #Print the final answer vector

